import pickle5 as pickle
import torch
import torch.nn as nn
from torch.nn.functional import dropout
import numpy as np
import math
import matplotlib.pyplot as plt
from CNN import CNN
import os
# $$
# result
out_dof_list = [2]
# input dof
in_dof_list = [0, 1, 2]

scr = 1000
# %% read data
path_of_folder = "/home/hao/Desktop/Hao/good_data"
files = os.listdir( path_of_folder )
desired_path_list = []
desired_velocity_list = []
desired_acceleration_list = []
ff_list = []
linear_model_list = []

max_dof = np.zeros( len(in_dof_list) )
min_dof = np.zeros( len(in_dof_list) )

max_ff = np.zeros( len(in_dof_list) )
min_ff = np.zeros( len(in_dof_list) )

flag = False

for file in files:
    if not os.path.isdir( file ):
        path_of_file = path_of_folder + '/' + file
        f = open(path_of_file, 'rb')
        t_stamp_u = pickle.load( f ) # time stamp for x-axis
        y_history = pickle.load( f ) 
        ff_history = pickle.load( f )
        angle_initial = pickle.load( f ) 
        disturbance_history = pickle.load( f )  
        P_history = pickle.load( f )
        time_history = pickle.load( f ) 
        f.close()
    # list of desired paths
    desired_path = y_history[0][in_dof_list, :] * 180 / math.pi

    ff_temp = disturbance_history[-1]
    linear_temp = ff_history[0]

    # normalization

    if flag == False:
        flag = True
        for dof in in_dof_list:
            max_dof[dof] = desired_path[dof, :].max()
            min_dof[dof] = desired_path[dof, :].min()

            max_ff[dof] = ff_temp[dof, :].max()
            min_ff[dof] = ff_temp[dof, :].min()
            
    else:
        for dof in in_dof_list:
            max_temp = desired_path[dof, :].max()
            min_temp = desired_path[dof, :].min()

            max_ff_temp = ff_temp[dof, :].max()
            min_ff_temp = ff_temp[dof, :].min()
            
            if max_temp > max_dof[dof]:
                max_dof[dof] = max_temp
            if min_temp < min_dof[dof]:
                min_dof[dof] = min_temp
            
            if max_ff_temp > max_ff[dof]:
                max_ff[dof] = max_ff_temp
            if min_ff_temp < min_ff[dof]:
                min_ff[dof] = min_ff_temp

    desired_path_list.append( desired_path )
    
    ff_list.append( ff_temp )
    # list of initial value generated by linear model
    linear_model_list.append( linear_temp[in_dof_list, :] )

for i in range( len(desired_path_list) ):
    desired_path = desired_path_list[i]
    ff_temp = ff_list[i]
    for dof in out_dof_list:
        ff_temp[dof, :] = ff_temp[dof, :] / (max_ff[dof] - min_ff[dof])
    ff_list[i] = ff_temp[out_dof_list, :]

    for dof in in_dof_list:
        desired_path[dof, :] = desired_path[dof, :] / (max_dof[dof] - min_dof[dof])
    desired_path_list[i] = desired_path

    desired_velocity = np.array([])
    desired_acceleration = np.array([])

    for index in range( desired_path.shape[1] ):
        if index == 0:
            desired_velocity = np.zeros( len(in_dof_list ) ).reshape(-1, 1)
            desired_acceleration = np.zeros( len(in_dof_list ) ).reshape(-1, 1)
        else:
            desired_velocity = np.hstack((desired_velocity, (desired_path[:, index].reshape(-1, 1) - desired_path[:, index-1].reshape(-1, 1)) / 0.01 ))
            desired_acceleration = np.hstack((desired_acceleration, (desired_velocity[:, index].reshape(-1, 1) - desired_velocity[:, index-1].reshape(-1, 1)) / 0.01 ))

    desired_velocity_list.append( desired_velocity )
    desired_acceleration_list.append( desired_acceleration )
# %% set some constants
# total number of training paths
number_path = len( desired_path_list )
# half length of inputs
q_input = 25
# half length of outputs
q_output = 2
# position of middle point (main point)
mid_position = q_output
# size of filter = degrees of freedom
filter_size = 7
# two channels: desired path and initial feedforward
channel_in = 1
channel_out = len( out_dof_list )
width_in = len( in_dof_list )
width_out = 1
# length of inputs
length_of_input = 2 * q_input + 1
# length of labels
length_of_output = 2 * q_output + 1
# how big part is used to train CNN model
k = 0.8
batch_size = 20
learning_rate = 1e-4
weight_decay = 0.00
dropout = 0.05
epoches = 500
# %% generate the dataset and labelset
dataset = []
labelset = []

train_data = []
train_label = []
val_data = []
val_label = []

for index in range( number_path ):
    # the dimensions should be 4 * n
    desired_path = desired_path_list[index].T
    desired_velocity = desired_velocity_list[index].T
    desired_acceleration = desired_acceleration_list[index].T
    initial_feedforward = linear_model_list[index].T
    ff = ff_list[index].T
    
    l = desired_path.shape[0]

    for i in range( l ):
        y_temp = np.array([])
        v_temp = np.array([])
        a_temp = np.array([])
        u_temp = np.array([])
        u_ini_temp = np.array([])
        # loop for inputs
        for j in range( i-q_input, i+q_input+1):
            if j < 0:
                y_temp = np.append(y_temp, desired_path[0, :] )
                v_temp = np.append(v_temp, desired_velocity[0, :] )
                a_temp = np.append(a_temp, desired_acceleration[0, :] )
                # u_ini_temp = np.append(u_ini_temp, initial_feedforward[0, :] )
            elif j > l-1:
                y_temp = np.append(y_temp, desired_path[-1, :] )
                v_temp = np.append(v_temp, desired_velocity[-1, :] )
                a_temp = np.append(a_temp, desired_acceleration[-1, :] )
                # u_ini_temp = np.append(u_ini_temp, initial_feedforward[-1, :] )
            else:
                y_temp = np.append(y_temp, desired_path[j, :] )
                v_temp = np.append(v_temp, desired_velocity[j, :] )
                a_temp = np.append(a_temp, desired_acceleration[j, :] )
                # u_ini_temp = np.append(u_ini_temp, initial_feedforward[j, :] )
        # loop for outputs
        for j in range( i-q_output, i+q_output+1):
            if j < 0:
                u_temp = np.append(u_temp, ff[0, :] )
            elif j > l-1:
                u_temp = np.append(u_temp, ff[-1, :] )
            else:
                u_temp = np.append(u_temp, ff[j, :] )
        
        labelset.append( u_temp )
        dataset.append( y_temp )

num_point = len( labelset )
num_train = int( k * num_point )
num_val = num_point - num_train

arr = np.arange( num_point )
# if select training data randomly
# np.random.shuffle( arr )

for i in range( num_point ):
    if i < num_train:
        train_data.append( dataset[arr[i]] )
        train_label.append( labelset[arr[i]] )
    else:
        val_data.append( torch.tensor(dataset[arr[i]], dtype=float ).view(1, channel_in, length_of_input, width_in)) 
        val_label.append( torch.tensor(labelset[arr[i]], dtype=float ).view(1, channel_out, length_of_output, width_out))

train_loader = []

idx = 0
while 1:
    if idx + batch_size - 1 < num_train:
        data_temp = train_data[idx : idx+batch_size]
        label_temp = train_label[idx : idx+batch_size]

        batch_x = torch.tensor(data_temp, dtype=float).view(batch_size, channel_in, length_of_input, width_in)
        batch_y = torch.tensor(label_temp, dtype=float).view(batch_size, channel_out, length_of_output, width_out)

        train_loader.append( (batch_x, batch_y) )

        idx += batch_size
    else:
        break
# %% build the CNN model
iter_per_epoch = len(train_loader)

train_loss_history = []
val_loss_history = []

cnn_model = CNN( channel_in=channel_in,
                 channel_out=channel_out,
                 dropout=dropout,
                 length_data=length_of_input,
                 length_label=length_of_output,
                 filter_size=filter_size,
                 width=width_in )

print(cnn_model)

optimizer = torch.optim.Adam(cnn_model.parameters(),
                             lr=learning_rate, weight_decay=weight_decay)
loss_function = nn.HuberLoss() #(size_average=True)
# %% train the cnn model
for epoch in range(epoches):

    avg_train_loss = 0.0 # loss summed over epoch and averaged
    cnn_model.train()

    for (batch_data, batch_label) in train_loader:
        output = cnn_model(batch_data.float())
        loss = loss_function(output.squeeze(), batch_label.view(batch_size, -1).float())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        avg_train_loss += loss.item()
    
    avg_train_loss /= iter_per_epoch
    train_loss_history.append(avg_train_loss) 
    # print ('\n[Epoch {}/{}] TRAIN loss: {:.3f}'.format(epoch+1, epoches, avg_train_loss))

    avg_eval_loss = 0.0
    cnn_model.eval()

    for i in range( len(val_label) ):      
        preds = cnn_model(val_data[i].float())
        loss = loss_function(preds.squeeze(), val_label[i].view(1, -1).float())
        avg_eval_loss += loss.item()
 
    avg_eval_loss /= len(val_label)
    val_loss_history.append(avg_eval_loss)
    print ('\n[Epoch {}/{}] TRAIN/VALID loss: {:.6}/{:.6f}'.format(epoch+1, epoches, avg_train_loss, avg_eval_loss))
# %%
plt.figure(1)
plt.plot(train_loss_history, '-o', label='Training')
plt.plot(val_loss_history, '-o', label='Evaluation')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(ncol=2, loc='upper center')
plt.show()

# %%
u_model = np.array([])
u_eval = np.array([])
t_stamp = np.linspace(0, (num_val - 1) / 100, num=num_val, endpoint=True)

for i in range( len(val_label) ):
    output = cnn_model(val_data[i].float())
    output = output.detach().numpy()
    output = output.reshape(1, channel_out, length_of_output, width_out)
    u_model = np.append(u_model, output[0, :, mid_position, 0])
    u_eval = np.append( u_eval, val_label[i][0, :, mid_position, 0])

u_model = u_model.reshape(-1, channel_out).T
u_eval = u_eval.reshape(-1 ,channel_out).T


Location = 'upper center'

index = 0
for dof in out_dof_list:

    line = []
    plt.figure( r'Prediction for dof: {}'.format(dof) )
    plt.xlabel(r'time t in s')
    plt.ylabel(r'Feedforward control')
    line_temp, = plt.plot(t_stamp, u_model[index, :] * (max_ff[dof] - min_ff[dof]), linewidth=1, label=r'Prediction, dof {}'.format(dof))
    line.append( line_temp )
    line_temp, = plt.plot(t_stamp, u_eval[index, :] * (max_ff[dof] - min_ff[dof]), linewidth=1.5, linestyle='--', label=r'Desired, dof {}'.format(dof))
    line.append( line_temp )
    plt.legend(handles = line, loc=Location, shadow=True)
    plt.show()

    index += 1
    
# # %%
# u_model_0 = np.array([])
# t_stamp_0 = np.linspace(0, (l_of_y_0-1)/100, num=l_of_y_0, endpoint=True)

# for data_y in y_eval_0:
#     output = cnn_model(data_y.float())
#     u_model_0 = np.append(u_model_0, np.asscalar(output[0][mid_position]) )

# plt.figure(3)
# plt.plot(t_stamp_0, u_model_0, linewidth=1, label='Prediction')
# plt.plot(t_stamp_0, u_eval_0, linewidth=1, label='Desired')
# plt.xlabel('t')
# plt.ylabel('Control')
# plt.legend(ncol=2, loc='upper center')
# plt.show()

# # %%
# u_model_1 = np.array([])
# t_stamp_1 = np.linspace(0, (l_of_y_1-1)/100, num=l_of_y_1, endpoint=True)

# for data_y in y_eval_1:
#     output = cnn_model(data_y.float())
#     u_model_1 = np.append(u_model_1, np.asscalar(output[0][mid_position]) )

# plt.figure(4)
# plt.plot(t_stamp_1, u_model_1, linewidth=1, label='Prediction')
# plt.plot(t_stamp_1, u_eval_1, linewidth=1, label='Desired')
# plt.xlabel('t')
# plt.ylabel('Control')
# plt.legend(ncol=2, loc='upper center')
# plt.show()